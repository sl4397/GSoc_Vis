shiny::runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
runApp()
library(lubridate)
library(tidyverse)
data <- read.csv("/Users/ShuangShuang/Desktop/GSoC_2019/data/GlobalLandTemperaturesByCountry.csv")
data$dt <- as.Date(data$dt)
data <- data %>%
mutate(year = year(dt)) %>%
group_by(Country, year) %>%
summarise(AvgTemp = mean(AverageTemperature, na.rm = T),
AvgUncertainty = mean(AverageTemperatureUncertainty, na.rm = T))
data <- data %>%
mutate(UpperQuantile = AvgTemp - AvgUncertainty/2) %>%
mutate(LowerQuantile = AvgTemp + AvgUncertainty/2)
write.csv(data, file="../output/CleanedTemperaturesByCountry.csv", row.names=FALSE)
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
data <- read.csv("../GlobalLandTemperaturesByCountry.csv")
data1 <- read.csv("../output/data1.csv")
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
shiny::runApp()
shiny::runApp()
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
shiny::runApp()
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
runApp()
runApp()
runApp()
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
shiny::runApp()
View(data)
runApp()
runApp()
runApp()
tempdata <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
tempdata <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
runApp()
runApp()
shiny::runApp()
data1 <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
runApp()
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp('~/Desktop/untitled folder')
runApp('~/Desktop/untitled folder')
runApp()
data <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
runApp('~/Desktop/untitled folder')
runApp()
shiny::runApp()
runApp()
runApp()
temperature <- read.csv("../output/CleanedTemperaturesByCountry.csv")
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
temperature <- read.csv("..output/CleanedTemperaturesByCountry.csv")
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
set.seed(2018)
setwd("/Users/ShuangShuang/Documents/GitHub/Spring2019-Proj3-spring2019-proj3-grp11")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located.
# use relative path for reproducibility
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
extra_label <- read.csv(train_label_path, colClasses=c("NULL", NA, NA))
source("../lib/feature_new.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
feat_train <- dat_train$feature
label_train <- dat_train$label
}
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
train_LR_dir
### load libraries
library("EBImage")
source('~/Documents/GitHub/Spring2019-Proj3-spring2019-proj3-grp11/lib/feature_new.R', echo=TRUE)
#############################################################
### Construct features and responses for training images###
#############################################################
### Authors: Zixiao Wang
### Project 3
# functions for helping getting feature
get_pixel <- function(IMG, row, col){
if(row >=1 && row <= dim(IMG)[1] && col >= 1 && col <= dim(IMG)[2]){
return(as.numeric(IMG[row, col]))
}
else{
return(0)
}
}
## LR
## x1  x4  x6
## x2  ct  x7
## x3  x5  x8
## HR
## y2  y1
## y3  y4
neighbor_and_label <- function(Index, LR_Data, HR_Data, Sample){
row <- arrayInd(Sample[Index], dim(LR_Data))[1]
col <- arrayInd(Sample[Index], dim(LR_Data))[2]
x1 <- get_pixel(LR_Data, row-1, col-1)
x2 <- get_pixel(LR_Data, row, col-1)
x3 <- get_pixel(LR_Data, row+1, col-1)
x4 <- get_pixel(LR_Data, row-1, col)
ct <- get_pixel(LR_Data, row, col)
x5 <- get_pixel(LR_Data, row+1, col)
x6 <- get_pixel(LR_Data, row-1, col+1)
x7 <- get_pixel(LR_Data, row, col+1)
x8 <- get_pixel(LR_Data, row+1, col+1)
neighbor_value <- c(x1, x2, x3, x4, x5, x6, x7, x8)
neighbor_value <- neighbor_value - ct
y1 <- get_pixel(HR_Data, 2*row-1, 2*col-1)
y2 <- get_pixel(HR_Data, 2*row, 2*col-1)
y3 <- get_pixel(HR_Data, 2*row-1, 2*col)
y4 <- get_pixel(HR_Data, 2*row, 2*col)
label_pixels <- c(y1, y2, y3, y4)
label_pixels <- label_pixels - ct
return(list(neighbor = neighbor_value, labelpix = label_pixels))
}
get_feature <- function(LR, HR, Channel, n){
LR_channel <- LR[ , , Channel]
HR_channel <- HR[ , , Channel]
Sample_Points <- sample(c(1:length(LR_channel)), n)
neighbor_value <- matrix(nrow = n, ncol = 8)
sub_pixels <- matrix(nrow = n, ncol = 4)
for (k in c(1:n)) {
Result <- neighbor_and_label(k, LR_channel, HR_channel, Sample_Points)
neighbor_value[k, ] <- Result$neighbor
sub_pixels[k,] <- Result$labelpix
}
return(list(neighbor_value = neighbor_value, label = sub_pixels))
}
feature <- function(LR_dir, HR_dir, n_points=1000){
### load libraries
library("EBImage")
n_files <- length(list.files(LR_dir))
### store feature and responses
featMat <- array(NA, c(n_files * n_points, 8, 3))
colnames(featMat) <- c("v1","v2","v3","v4","v5","v6","v7","v8")
labMat <- array(NA, c(n_files * n_points, 4, 3))
colnames(labMat) <- c("v1","v2","v3","v4")
### read LR/HR image pairs
for(i in 1:n_files){
imgLR <- readImage(paste0(LR_dir,  "img_", sprintf("%04d", i), ".jpg"))
imgHR <- readImage(paste0(HR_dir,  "img_", sprintf("%04d", i), ".jpg"))
imgLR <- as.array(imgLR)
imgHR <- as.array(imgHR)
### step 1. sample n_points from imgLR
### step 2. for each sampled point in imgLR,
### step 2.1. save (the neighbor 8 pixels - central pixel) in featMat
###           tips: padding zeros for boundary points
### step 2.2. save the corresponding 4 sub-pixels of imgHR in labMat
### step 3. repeat above for three channels
for(c in 1:3){
featMat[(n_points*(i-1)+1):(n_points*i), , c] <- get_feature(imgLR, imgHR, c, n_points)$neighbor_value
labMat[(n_points*(i-1)+1):(n_points*i), , c] <- get_feature(imgLR, imgHR, c, n_points)$label
}
}
return(list(feature = featMat, label = labMat))
}
source("../lib/feature_new.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
feat_train <- dat_train$feature
label_train <- dat_train$label
}
#save(dat_train, file="./output/feature_train.RData")
tm_feature_train
feat_train
i = 1:12
i
(i-1) %% 4 + 1
(i-c1) %/% 4 + 1
c1 <- (i-1) %% 4 + 1
(i-c1) %/% 4 + 1
install.packages("xgboost")
library(xgboost)
?xgboost
######################################################
### Fit the regression model with testing data ###
######################################################
### Project 3
test <- function(modelList, dat_test){
### Fit the classfication model with testing data
### Input:
###  - the fitted classification model list using training data
###  - processed features from testing images
### Output: training model specification
### load libraries
library("xgboost")
predArr <- array(NA, c(dim(dat_test)[1], 4, 3))
for (i in 1:12){
fit_train <- modelList[[i]]
### calculate column and channel
c1 <- (i-1) %% 4 + 1
c2 <- (i-c1) %/% 4 + 1
featMat <- dat_test[, , c2]
### make predictions
predArr[, c1, c2] <- predict(fit_train$fit, newdata=featMat, type="response")
}
return(as.numeric(predArr))
}
library(tidyverse)
library(leaps)
library(e1071)
nonlinear.fit <- svm(Y.train ~ ., zip.train, cost = 1, gamma = 0.01, scale = F)
library(tidyverse)
library(leaps)
library(e1071)
credit <- read.csv("/Users/ShuangShuang/Desktop/5241_ML/hw4/Credit.csv")
credit <- credit[,-1]
best <- regsubsets(Balance ~ . - Balance, data = credit)
forward <- regsubsets(Balance ~ . - Balance, data = credit, method = "forward")
backward <- regsubsets(Balance ~ . - Balance, data = credit, method = "backward")
x <- rep(c(1:11), 3)
rss <- c(best$rss[-1], forward$rss[-1], backward$rss[-1])
model <- rep(c("best_subset", "forward_stepwise", "backward_stepwise"), each = 11)
rss.df <- data.frame(x, rss, model)
ggplot(rss.df, aes(x = x, y = rss, color = model))+
geom_point()+
geom_line(alpha = 0.7)+
scale_x_continuous(breaks = c(1:11))+
xlab("# of Variables")
best.sum <- summary(best)
forward.sum <- summary(forward)
backward.sum <- summary(backward)
# Cp criterior
best.sum$which[which.min(best.sum$cp),]
forward.sum$which[which.min(forward.sum$cp),]
backward.sum$which[which.min(backward.sum$cp),]
# BIC criterior
best.sum$which[which.min(best.sum$bic),]
forward.sum$which[which.min(forward.sum$bic),]
backward.sum$which[which.min(backward.sum$bic),]
zip.5<-read.table("train.5-1.txt", header=FALSE, sep=",")
zip.6<-read.table("train.6.txt", header=FALSE, sep=",")
zip <- rbind(zip.5, zip.6)
Y <- c(rep(-1, nrow(zip.5)), rep(1, nrow(zip.6)))
set.seed(0)
n <- nrow(zip)
test <- sample(1:n, n*0.2)
zip.test <- zip[test,]
Y.test <- Y[test]
zip.train <- zip[-test,]
Y.train <- Y[-test]
cv_error=function(fitted_model,newy,newx){
pred <- predict(fitted_model, newx)
Y.pred <- ifelse(pred<0, -1, 1)
error <- mean(Y.pred != newy)
return(error)
}
set.seed(0)
fold <- sample(rep(c(1:8), 122), 976)
cost <- c(0.0001, 0.001, 0.01, 0.1)
error.rate <- numeric(length(cost))
count = 1
for(c in cost){
error <- numeric(8)
for( k in 1:8){
fit <- svm(Y.train[fold != k ] ~ ., zip.train[fold != k, ],
kernel = "linear", cost = c, scale = F)
error[k] <- cv_error(fit, Y.train[fold == k], zip.train[fold == k, ])
}
error.rate[count] <- mean(error)
count = count + 1
}
error.rate
df1 <- data.frame(cost, error.rate)
ggplot(df1, aes(cost, error.rate))+
geom_point()+
geom_line()+
scale_x_log10()+
ggtitle("linear SVM with soft margin")+
ylab("misclassification rate")
cost <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000)
bandwidth <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000)
error.rate2 <- matrix(NA, nrow = length(cost), ncol = length(bandwidth))
i = 1
for(c in cost){
j = 1
for(b in bandwidth){
error <- numeric(8)
for( k in 1:8){
fit <- svm(Y.train[fold != k ] ~ ., zip.train[fold != k, ],
cost = c, scale = F, gamma = b)
error[k] <- cv_error(fit, Y.train[fold == k], zip.train[fold == k, ])
}
error.rate2[i,j] <- mean(error)
j = j+1
}
i = i+1
}
colnames(error.rate2) <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000)
rownames(error.rate2) <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000)
error.rate2
df2 <- data.frame(cost = rep(c(0.001, 0.01, 0.1, 1, 10, 100, 1000), each = 7),
bandwidth = rep(c(0.001, 0.01, 0.1, 1, 10, 100, 1000),7),
z = as.vector(t(error.rate2)))
ggplot(df2, aes(x = bandwidth, y = cost, fill = z), color = white)+
geom_tile()+
scale_x_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
label = c("0.001"," 0.01", "0.1", "1", "10", "100", "1000"))+
scale_y_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
label = c("0.001"," 0.01", "0.1", "1", "10", "100", "1000"))+
scale_fill_viridis_c()
linear.fit <- svm(Y.train ~ ., zip.train, kernel = "linear", cost = 0.001, scale = F)
linear.error <- cv_error(linear.fit, Y.test, zip.test)
linear.error
nonlinear.fit <- svm(Y.train ~ ., zip.train, cost = 1, gamma = 0.01, scale = F)
nonlinear.error <- cv_error(nonlinear.fit, Y.test, zip.test)
nonlinear.error
nonlinear.fit <- svm(Y.train ~ ., zip.train, cost = 2, gamma = 0.01, scale = F)
nonlinear.error <- cv_error(nonlinear.fit, Y.test, zip.test)
nonlinear.error
nonlinear.fit <- svm(Y.train ~ ., zip.train, cost = 2, gamma = 0.01)
nonlinear.error <- cv_error(nonlinear.fit, Y.test, zip.test)
nonlinear.error
nonlinear.fit <- svm(Y.train ~ ., zip.train, cost = 2, gamma = 0.01, scale = F,
kernal = "radial")
nonlinear.error <- cv_error(nonlinear.fit, Y.test, zip.test)
nonlinear.error
nonlinear.fit <- svm(Y.train ~ ., zip.train, cost = 100, gamma = 0.001, scale = F)
nonlinear.error <- cv_error(nonlinear.fit, Y.test, zip.test)
nonlinear.error
